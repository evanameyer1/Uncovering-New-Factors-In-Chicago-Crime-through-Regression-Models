{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_feature_training_sample = pd.read_csv('../../data/pre_training/area_feature_training_sample.csv')\n",
    "area_target_training_sample = pd.read_csv('../../data/pre_training/area_target_training_sample.csv')\n",
    "district_feature_training_sample = pd.read_csv('../../data/pre_training/district_feature_training_sample.csv')\n",
    "district_target_training_sample = pd.read_csv('../../data/pre_training/district_target_training_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_feature_testing_data = pd.read_csv('../../data/pre_training/area_feature_testing_data.csv')\n",
    "area_target_testing_data = pd.read_csv('../../data/pre_training/area_target_testing_data.csv')\n",
    "district_feature_testing_data = pd.read_csv('../../data/pre_training/district_feature_testing_data.csv')\n",
    "district_target_testing_data = pd.read_csv('../../data/pre_training/district_target_testing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_feature_training_sample.drop('crime_status', axis=1, inplace=True)\n",
    "district_feature_training_sample.drop('crime_status', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dtrain_reg = xgb.DMatrix(area_feature_training_sample, area_target_training_sample, enable_categorical=True)\n",
    "area_dtest_reg = xgb.DMatrix(area_feature_testing_data, area_target_testing_data, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_dtrain_reg = xgb.DMatrix(district_feature_training_sample, district_target_training_sample, enable_categorical=True)\n",
    "district_dtest_reg = xgb.DMatrix(district_feature_testing_data, district_target_testing_data, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model, testing):\n",
    "        self.model = model\n",
    "        self.test_features = testing\n",
    "        self.sample_size = len(testing.get_label())\n",
    "        self.independent_vars = len(testing.feature_names)\n",
    "        self.predictions = None\n",
    "        self.residuals = None\n",
    "\n",
    "    def gather_predictions(self):\n",
    "        self.predictions = self.model.predict(self.test_features)\n",
    "        self.residuals = self.test_features.get_label() - self.predictions\n",
    "    \n",
    "    def mae(self):\n",
    "        if self.residuals is None:\n",
    "            return \"Predictions need to be gathered first\"\n",
    "        return np.mean(np.abs(self.residuals))\n",
    "    \n",
    "    def mse(self):\n",
    "        if self.residuals is None:\n",
    "            return \"Predictions need to be gathered first\"\n",
    "        return np.mean(self.residuals ** 2)\n",
    "    \n",
    "    def rmse(self):\n",
    "        if self.residuals is None:\n",
    "            return \"Predictions need to be gathered first\"\n",
    "        return np.sqrt(self.mse())\n",
    "    \n",
    "    def relative_rmse(self):\n",
    "        if self.residuals is None:\n",
    "            return \"Predictions need to be gathered first\"\n",
    "        return self.rmse() / max(self.test_features.get_label())\n",
    "\n",
    "    def r_squared(self):\n",
    "        if self.residuals is None:\n",
    "            return \"Predictions need to be gathered first\"\n",
    "        ss_res = np.sum(self.residuals ** 2)\n",
    "        ss_tot = np.sum((self.test_features.get_label() - np.mean(self.test_features.get_label())) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "    def adjusted_r_squared(self):\n",
    "        if self.residuals is None:\n",
    "            return \"Predictions need to be gathered first\"\n",
    "        r2 = self.r_squared()\n",
    "        return 1 - (1 - r2) * ((self.sample_size - 1) / (self.sample_size - self.independent_vars - 1))\n",
    "    \n",
    "    def median_absolute_error(self):\n",
    "        if self.residuals is None:\n",
    "            return \"Predictions need to be gathered first\"\n",
    "        return np.median(np.abs(self.residuals))\n",
    "\n",
    "    def feature_importances(self):\n",
    "        # Get feature importance based on different metrics\n",
    "        importance_weight = self.model.get_score(importance_type='weight')  # number of times a feature is used to split the data\n",
    "        importance_gain = self.model.get_score(importance_type='gain')  # improvement in accuracy of a feature to the branches it's on\n",
    "        importance_cover = self.model.get_score(importance_type='cover')  # average number of samples affected by a feature\n",
    "\n",
    "        # Sort and print feature importance based on weight\n",
    "        sorted_weight = sorted(importance_weight.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(\"Feature importance based on weight (sorted):\", sorted_weight)\n",
    "\n",
    "        # Sort and print feature importance based on gain\n",
    "        sorted_gain = sorted(importance_gain.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(\"Feature importance based on gain (sorted):\", sorted_gain)\n",
    "\n",
    "        # Sort and print feature importance based on cover\n",
    "        sorted_cover = sorted(importance_cover.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(\"Feature importance based on cover (sorted):\", sorted_cover)\n",
    "\n",
    "        return importance_weight, importance_gain, importance_cover\n",
    "\n",
    "    def plot_residuals(self, title, save_name):\n",
    "        # Calculate percent differences\n",
    "        percent_diff = (self.residuals / max(self.test_features.get_label())) * 100\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(percent_diff, label='Percent Difference', marker='o')\n",
    "        \n",
    "        # Calculate the average percent difference and plot it\n",
    "        avg_percent_diff = np.mean(percent_diff)\n",
    "        plt.axhline(y=avg_percent_diff, color='r', linestyle='dashed', label='Average Percent Difference')\n",
    "        \n",
    "        plt.text(0, avg_percent_diff + 1, f'Avg: {avg_percent_diff:.5f}%', color='r', fontsize=12)\n",
    "\n",
    "        plt.title(f'Percent Difference between Test Labels and Predicted Values - {title} Model')\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Percent Difference (%)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'../results/xgboost/{save_name}.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_labels, title=None, save_name=None):\n",
    "    ev = Evaluator(model, test_labels)\n",
    "    ev.gather_predictions()\n",
    "    \n",
    "    print(f\"Mean Absolute Error (MAE): {ev.mae():.4f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {ev.mse():.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {ev.rmse():.4f}\")\n",
    "    print(f\"Relative Root Mean Squared Error (Relative RMSE): {ev.relative_rmse():.4f}\")\n",
    "    print(f\"R-squared (RÂ²): {ev.r_squared():.4f}\")\n",
    "    print(f\"Adjusted R-squared: {ev.adjusted_r_squared():.4f}\")\n",
    "    print(f\"Median Absolute Error: {ev.median_absolute_error():.4f}\")\n",
    "    \n",
    "    importance_weight, importance_gain, importance_cover = ev.feature_importances()\n",
    "\n",
    "    if title is not None:\n",
    "        ev.plot_residuals(title, save_name)\n",
    "\n",
    "    return importance_weight, importance_gain, importance_cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the Area XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_params = {\"objective\": \"reg:squarederror\", \"device\":\"gpu\", \"eta\":\"0.001\"}\n",
    "area_evals = [(area_dtest_reg, \"validation\"), (area_dtrain_reg, \"train\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_model = xgb.train(\n",
    "   params=area_params,\n",
    "   dtrain=area_dtrain_reg,\n",
    "   num_boost_round=10000000,\n",
    "   evals=area_evals,\n",
    "   verbose_eval=1000,\n",
    "   early_stopping_rounds=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_importance_weight, area_importance_gain, area_importance_cover = evaluate(area_model, area_dtest_reg, 'Area', 'area_residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the District XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_params = {\"objective\": \"reg:squarederror\", \"device\":\"gpu\", \"eta\":\"0.001\"}\n",
    "district_evals = [(district_dtest_reg, \"validation\"), (district_dtrain_reg, \"train\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_model = xgb.train(\n",
    "   params=district_params,\n",
    "   dtrain=district_dtrain_reg,\n",
    "   num_boost_round=10000000,\n",
    "   evals=district_evals,\n",
    "   verbose_eval=1000,\n",
    "   early_stopping_rounds=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_importance_weight, district_importance_gain, district_importance_cover = evaluate(district_model, district_dtest_reg, 'District', 'district_residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_importances(importances):\n",
    "    total = sum(importances.values())    \n",
    "    return {key : val / total for key, val in importances.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(importances, type, threshold):\n",
    "    if type == 'cum_sum':\n",
    "        sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
    "        cumulative_sum = 0\n",
    "        out = []\n",
    "        for feature, importance in sorted_features:\n",
    "            cumulative_sum += importance\n",
    "            out.append(feature)\n",
    "            if cumulative_sum >= threshold:\n",
    "                break\n",
    "        return out\n",
    "    else:\n",
    "        return [key for key, val in importances.items() if val >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(model, type, training_sample, testing_data, thresholds):\n",
    "    weight_importances = normalize_importances(model.get_score(importance_type='weight'))\n",
    "    gain_importances = normalize_importances(model.get_score(importance_type='gain'))\n",
    "    cover_importances = normalize_importances(model.get_score(importance_type='cover'))    \n",
    "    agg_importances = {feature : np.mean((weight_importances[feature], gain_importances[feature], cover_importances[feature])) for feature in weight_importances.keys()}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        target_features = extract_features(agg_importances, type, threshold)\n",
    "        temp_dtrain = xgb.DMatrix(training_sample[0][target_features], training_sample[1], enable_categorical=True)\n",
    "        temp_dtest = xgb.DMatrix(testing_data[0][target_features], testing_data[1], enable_categorical=True)\n",
    "        temp_model = xgb.train(\n",
    "            params={\"objective\": \"reg:squarederror\", \"device\":\"gpu\", \"eta\":\"0.001\"},\n",
    "            dtrain=temp_dtrain,\n",
    "            num_boost_round=10000000,\n",
    "            evals=[(temp_dtest, \"validation\"), (temp_dtrain, \"train\")],\n",
    "            verbose_eval=False,\n",
    "            early_stopping_rounds=100\n",
    "        )\n",
    "        print(f'Threshold of {threshold} resulted in the features: {target_features}')\n",
    "        evaluate(temp_model, temp_dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection(district_model, 'cum_sum', (district_feature_training_sample, district_target_training_sample), (district_feature_testing_data, district_target_testing_data), np.arange(0.9, 1, 0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection(district_model, 'cum_sum', (district_feature_training_sample, district_target_training_sample), (district_feature_testing_data, district_target_testing_data), np.arange(0.92, 1, 0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection(area_model, 'cum_sum', (area_feature_training_sample, area_target_training_sample), (area_feature_testing_data, area_target_testing_data), np.arange(0.9, 1, 0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection(area_model, 'cum_sum', (area_feature_training_sample, area_target_training_sample), (area_feature_testing_data, area_target_testing_data), np.arange(0.88, 1, 0.04))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_district = {\n",
    "    'n_estimators': [10000],\n",
    "    'max_depth': np.arange(3, 20, 2),  # Explore deeper trees for potential complex patterns\n",
    "    'min_child_weight': np.arange(1, 20, 2),  # Allow for more conservative and less conservative splits\n",
    "    'gamma': np.linspace(0, 1.0, 10),  # Increase gamma range to explore more conservative split criteria\n",
    "    'subsample': np.linspace(0.3, 1.0, 10),  # Lower the minimum subsample to add more randomness\n",
    "    'colsample_bytree': np.linspace(0.3, 1.0, 10),  # Lower the minimum colsample_bytree to add more randomness\n",
    "    'eta': [0.01],\n",
    "    'objective':['reg:squarederror'],\n",
    "    'device':['gpu']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_district = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_district = RandomizedSearchCV(\n",
    "    estimator=model_district,\n",
    "    param_distributions=param_grid_district,\n",
    "    n_iter=200,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_district.fit(district_feature_training_sample, district_target_training_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters for District Model:\", random_search_district.best_params_)\n",
    "print(\"Best Score for District Model:\", random_search_district.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_district = {\n",
    "    'n_estimators': [10000],\n",
    "    'max_depth': [19, 21, 23, 25, None],\n",
    "    'min_child_weight': [1.0], \n",
    "    'gamma': [0.0], \n",
    "    'subsample': [0.8, 0.85, 0.9],\n",
    "    'colsample_bytree': [0.65, 0.7, 0.75], \n",
    "    'eta': [0.01],\n",
    "    'objective': ['reg:squarederror'],\n",
    "    'device': ['gpu']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_district = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_district = GridSearchCV(\n",
    "    estimator=model_district,\n",
    "    param_grid=param_grid_district,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_district.fit(district_feature_training_sample, district_target_training_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_area = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_area = {\n",
    "    'n_estimators': [20000],\n",
    "    'max_depth': np.arange(3, 20, 2),\n",
    "    'min_child_weight': np.arange(1, 20, 2),\n",
    "    'gamma': np.linspace(0, 1.0, 10),\n",
    "    'subsample': np.linspace(0.3, 1.0, 10), \n",
    "    'colsample_bytree': np.linspace(0.3, 1.0, 10),\n",
    "    'eta': [0.01],\n",
    "    'objective':['reg:squarederror'],\n",
    "    'device':['gpu']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_area = RandomizedSearchCV(\n",
    "    estimator=model_area,\n",
    "    param_distributions=param_grid_area,\n",
    "    n_iter=200,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_area.fit(area_feature_training_sample, area_target_training_sample)\n",
    "\n",
    "print(\"Best Parameters for Area Model:\", random_search_area.best_params_)\n",
    "print(\"Best Score for Area Model:\", random_search_area.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_proj",
   "language": "python",
   "name": "ml_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
